---
title: "Net Neutrality Analytics"
author: "Joshua Gonzalez"
date: "12/4/2017"
output: html_document
---

##Setting Up Authentication for TwitteR App
```{r}
if (!require(twitteR)) {install.packages("twitteR")}
if (!require(ROAuth)) {install.packages("ROAuth")}
library(twitteR)
library(ROAuth)

my.consumer.key = "bCrWKDSEgeh1Xn789rMUHvZKv"
my.consumer.secret = "smUlzMxjydfY6B2LX5G8Bb5kh7B4NKFswsWa3wFDT3wjuMCBUG"
my.access.token = "918094176675356672-T78leujAUHUsZ2qlGtUYjqEdNtLz6Vo"
my.access.token.secret = "SNirzHc9EOLbM3n5FCpRLTLRdreZF6VgE2h9sEgGlYWXf"
my_oauth <- setup_twitter_oauth(consumer_key = my.consumer.key, consumer_secret = my.consumer.secret, access_token = my.access.token, access_secret = my.access.token.secret)

save(my_oauth, file = "my_oauth.Rdata")
```

##Pulling Tweets on Net Neutrality From Twitter
```{r}
search.string <- "#Netneutrality"
result.term <- searchTwitter(search.string, n = 100)
head(result.term)

df.term <- twListToDF(result.term)
write.csv(df.term, "Netneutrality.csv")
```

##Pulling Data from NYC on Net Neutrality
```{r}
result.latlon <- searchTwitter('netneutrality', geocode='40.7128,-74.0060,20mi', n = 100)
head(result.latlon)

df.latlon <- twListToDF(result.latlon)
write.csv(df.latlon, "NetNeutralityNYC.csv")
```

##Creating a Word Cloud from NYC Data
```{r}
if (!require(twitteR)) {install.packages("twitteR")}
if (!require(ROAuth)) {install.packages("ROAuth")}
library(twitteR)
library(ROAuth)
library(tm)
library(leaflet)
library(plyr)
library(dplyr)
library(stringr)
library(ggplot2)
library(wordcloud)


my.consumer.key = "bCrWKDSEgeh1Xn789rMUHvZKv"
my.consumer.secret = "smUlzMxjydfY6B2LX5G8Bb5kh7B4NKFswsWa3wFDT3wjuMCBUG"
my.access.token = "918094176675356672-T78leujAUHUsZ2qlGtUYjqEdNtLz6Vo"
my.access.token.secret = "SNirzHc9EOLbM3n5FCpRLTLRdreZF6VgE2h9sEgGlYWXf"
my_oauth <- setup_twitter_oauth(consumer_key = my.consumer.key, consumer_secret = my.consumer.secret, access_token = my.access.token, access_secret = my.access.token.secret)

tweets <- searchTwitter("#netneutrality OR Net Neutrality", n=1000, lang="en")
tweets.text <- sapply(tweets, function(x) x$getText())


tweets.text <- gsub("rt", "", tweets.text)
tweets.text <- gsub("@\\w+", "", tweets.text)
tweets.text <- gsub("[[:punct:]]", "", tweets.text)
tweets.text <- gsub("http\\w+", "", tweets.text)
tweets.text <- gsub("[ |\t]{2,}", "", tweets.text)
tweets.text <- gsub("^ ", "", tweets.text)
tweets.text <- gsub(" $", "", tweets.text)
tweets.text <- gsub(" [^[:graph:]]", " ", tweets.text) 
tweets.text <- gsub(" [^[:digit:]]+", " ", tweets.text) 
tweets.text <- gsub("[^[:graph:]]", " ", tweets.text)
tweets.text <- tolower(tweets.text)


if(!require(tm)) {install.packages("tm")}
library(tm)
#create corpus
tweets.text.corpus <- Corpus(VectorSource(tweets.text))
#clean up by removing stop words
tweets.text.corpus <- tm_map(tweets.text.corpus, function(x) removeWords(x,stopwords()))

if(!require(wordcloud)) {install.packages("wordcloud")}
library(wordcloud)

wordcloud(tweets.text.corpus,min.freq = 2, scale=c(7,0.75),colors=brewer.pal(8, "Dark2"),  random.color= TRUE, random.order = FALSE, max.words = 150)
```
#Created a Map from NYC CSV and LatLong Data
```{r}
New_York <- read.csv("NetNeutralityNYC.csv")
nyleaf <- leaflet(width = "100%") %>%
  addTiles() %>%
  addProviderTiles(provider = "Esri.WorldStreetMap",group = "World StreetMap") %>%
  addProviderTiles(provider = "Esri.WorldImagery",group = "World Imagery") %>%
  addLayersControl(
    baseGroups = c("OSM (default)","World StreetMap", "World Imagery"),
    options = layersControlOptions(collapsed = FALSE)) %>%
  addMarkers(label = New_York$text, lng = 40.7128, lat= -74.0060, clusterOptions = markerClusterOptions())
nyleaf
```

##Created Sentiment Analyis on Net Neutrality
```{r}
if (!require(twitteR)) {install.packages("twitteR")}
if (!require(ROAuth)) {install.packages("ROAuth")}
if (!require(plyr)) {install.packages("plyr")}
if (!require(stringr)) {install.packages("stringr")}
if (!require(ggplot2)) {install.packages("ggplot2")}
library(twitteR)
library(ROAuth)
library(plyr)
library(dplyr)
library(stringr)
library(ggplot2)

my.consumer.key = "bCrWKDSEgeh1Xn789rMUHvZKv"
my.consumer.secret = "smUlzMxjydfY6B2LX5G8Bb5kh7B4NKFswsWa3wFDT3wjuMCBUG"
my.access.token = "918094176675356672-T78leujAUHUsZ2qlGtUYjqEdNtLz6Vo"
my.access.token.secret = "SNirzHc9EOLbM3n5FCpRLTLRdreZF6VgE2h9sEgGlYWXf"
my_oauth <- setup_twitter_oauth(consumer_key = my.consumer.key, consumer_secret = my.consumer.secret, access_token = my.access.token, access_secret = my.access.token.secret)

save(my_oauth, file = "my_oauth.Rdata")

neg <-  scan("negative-words.txt", what="character", comment.char=";")

pos <- scan("positive-words.txt", what="character", comment.char=";")

score.sentiment = function(tweets, pos.words, neg.words)
{
scores = laply(tweets, function(tweet, pos.words, neg.words) {
tweet = gsub('https://','',tweet) 
tweet = gsub('http://','',tweet) 
tweet=gsub('[^[:graph:]]', ' ',tweet) 
tweet = gsub('[[:punct:]]', '', tweet) 
tweet = gsub('[[:cntrl:]]', '', tweet)
tweet = gsub('\\d+', '', tweet) 
tweet=str_replace_all(tweet,"[^[:graph:]]", " ") 
tweet = tolower(tweet) 
word.list = str_split(tweet, '\\s+') 
words = unlist(word.list) 
 
pos.matches = match(words, pos.words) 
        
neg.matches = match(words, neg.words)
 
pos.matches = !is.na(pos.matches) 
neg.matches = !is.na(neg.matches)
 
score = sum(pos.matches) - sum(neg.matches) 
 
return(score)
 
}, pos.words, neg.words )
 
scores.df = data.frame(score=scores, text=tweets)
 
return(scores.df)
}

tweets = searchTwitter('netneutrality',n=2500)
Tweets.text = laply(tweets,function(t)t$getText()) 
analysis = score.sentiment(Tweets.text, pos, neg) 

table(analysis$score)

hist(analysis$score, main = "Sentiment Analysis", xlab = "Score")
```



